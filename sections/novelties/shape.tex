CNNs are well-known to be texture-biased~\cite{texture-bias-1,texture-bias-2,texture-bias-3,texture-bias-4,texture-bias-5}.
In our case, we observed this empirically in our model's predictions images on a pixel level.
We would like the model to be aware of the shapes of roads (e.g.\ roads are straight thick lines), and appropriately give predictions.

In the U-Net architecture, the outputs of the lowest level has a lower resolution as compared to the outputs of any other level.
Thus, each pixel in that level should encode coarse information about a patch of the input image.
Since textures are fine local details and shapes are coarse global details, we theorize that the U-Net bottleneck outputs should ideally contain shape information about the image.
However, this may not occur in practice, and it may lead to texture-biased models.

Therefore, we use a contrastive learning framework to enforce shape information in the U-Net bottleneck.
For this, we formulate the following constraint:

\newtheorem*{constraint}{Constraint}
\begin{constraint}
Any two images with the same shape, even with different textures, should give the same bottleneck output
\end{constraint}

The U-Net architecture contains multiple skip connections that can propagate information in the earlier layers.
Thus, even if texture information is lost in the bottleneck, it can be obtained through these layers.
We leave the choice of how much of each kind of information is used up to the right half of the architecture.

To get two images with the same shape but with different textures, we use certain augmentations that distort texture while preserving shape.
These are applied in the following order:
\begin{enumerate}
    \item Gaussian blur
    \item Nearest-neighbour downscaling followed by upscaling
    \item JPEG compression
    \item Gaussian noise
    \item Camera sensor noise a.k.a. ISO noise (using Possion noise)
\end{enumerate}

We use these augmentations with the SimCLR~\cite{simclr} framework.
Here, bottleneck outputs are thought of as latent vectors containing shape information.
In a nutshell, the shape latents for images with the same shape are forced to be closer, and those for different images are forced apart.
This is achieved using the following loss setup:

\begin{gather}
    \mathcal{L}_{shape} = - \sum_{(i,j) \in S} \log \frac{\exp(\text{cosine-sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau)}{\sum_{k \neq i} \exp(\text{cosine-sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau)} \\
    \mathcal{L}_{total} = \mathcal{L}_{cross-entropy} + \alpha_{shape} \mathcal{L}_{shape}
\end{gather}

Here, $S = \{ (i, j) | \mathbf{z}_i = \text{bottleneck}(\mathbf{x}_i), \mathbf{z}_j = \text{bottleneck}(\text{augmentations}(\mathbf{x}_i)) \}$ is the set of all latent vectors for images with the same shape but with different textures.
All images are taken from a mini-batch and are compared with each other (we assume that they have different shapes).
cosine-sim$(\mathbf{x}, \mathbf{y})$ is the cosine similarity between vectors $\mathbf{x}$ and $\mathbf{y}$.
$\tau$ is the temperature coefficient for the softmax.
