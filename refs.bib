@misc{unet,
      title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
      author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
      year={2015},
      eprint={1505.04597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{simclr,
      title={A Simple Framework for Contrastive Learning of Visual Representations}, 
      author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
      year={2020},
      eprint={2002.05709},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{resnet,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{batch-norm,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{dropout,
    author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
    title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
    year = {2014},
    issue_date = {January 2014},
    publisher = {JMLR.org},
    volume = {15},
    number = {1},
    issn = {1532-4435},
    abstract = {Deep neural nets with a large number of parameters are very powerful machine learning
    systems. However, overfitting is a serious problem in such networks. Large networks
    are also slow to use, making it difficult to deal with overfitting by combining the
    predictions of many different large neural nets at test time. Dropout is a technique
    for addressing this problem. The key idea is to randomly drop units (along with their
    connections) from the neural network during training. This prevents units from co-adapting
    too much. During training, dropout samples from an exponential number of different
    "thinned" networks. At test time, it is easy to approximate the effect of averaging
    the predictions of all these thinned networks by simply using a single unthinned network
    that has smaller weights. This significantly reduces overfitting and gives major improvements
    over other regularization methods. We show that dropout improves the performance of
    neural networks on supervised learning tasks in vision, speech recognition, document
    classification and computational biology, obtaining state-of-the-art results on many
    benchmark data sets.},
    journal = {J. Mach. Learn. Res.},
    month = jan,
    pages = {1929â€“1958},
    numpages = {30},
    keywords = {model combination, deep learning, regularization, neural networks}
}
