@article{nethack_paper,
  title={The nethack learning environment},
  author={K{\"u}ttler, Heinrich and Nardelli, Nantas and Miller, Alexander H and Raileanu, Roberta and Selvatici, Marco and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:2006.13760},
  year={2020}
}

@misc{nethack_wiki,
  title = {{NetHackWiki}},
  howpublished = "\url{https://nethackwiki.com/wiki/Main_Page}",
}

@misc{nethack_spoilers,
  title = {{List of NetHack Spoilers}},
  howpublished = "\url{https://sites.google.com/view/evasroguelikegamessite/list-of-nethack-spoilers}",
}

@misc{nethack_guide,
  title = {{A Guide to the Mazes of Menace (Guidebook for NetHack)}},
  howpublished = "\url{http://www.nethack.org/download/3.6.5/nethack-365-Guidebook.pdf}",
}

@article{Branavan_2012,
   title={Learning to Win by Reading Manuals in a Monte-Carlo Framework},
   volume={43},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.3484},
   DOI={10.1613/jair.3484},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Branavan, S.R.K. and Silver, D. and Barzilay, R.},
   year={2012},
   month={Apr},
   pages={661–704}
}

@misc{RTFM,
      title={RTFM: Generalising to Novel Environment Dynamics via Reading}, 
      author={Victor Zhong and Tim Rocktäschel and Edward Grefenstette},
      year={2021},
      eprint={1910.08210},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{EMMA,
      title={Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning}, 
      author={H. J. Austin Wang and Karthik Narasimhan},
      year={2021},
      eprint={2101.07393},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{luketina2019survey,
  title={A survey of reinforcement learning informed by natural language},
  author={Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:1906.03926},
  year={2019}
}

@misc{realm,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{LSTM,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@misc{RND,
      title={Exploration by Random Network Distillation}, 
      author={Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
      year={2018},
      eprint={1810.12894},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{morin2005hierarchical,
  title={Hierarchical probabilistic neural network language model.},
  author={Morin, Frederic and Bengio, Yoshua},
  booktitle={Aistats},
  volume={5},
  pages={246--252},
  year={2005},
  organization={Citeseer}
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@misc{big-bird,
  title={Big Bird: Transformers for Longer Sequences}, 
  author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  year={2021},
  eprint={2007.14062},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{transformer,
  title={Attention Is All You Need}, 
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year={2017},
  eprint={1706.03762},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding}, 
  author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  year={2020},
  eprint={1906.08237},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year={2019},
  eprint={1907.11692},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
  author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  year={2020},
  eprint={1909.11942},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{electra,
  title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
  author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  year={2020},
  eprint={2003.10555},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}


@misc{impala,
      title={IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures}, 
      author={Lasse Espeholt and Hubert Soyer and Remi Munos and Karen Simonyan and Volodymir Mnih and Tom Ward and Yotam Doron and Vlad Firoiu and Tim Harley and Iain Dunning and Shane Legg and Koray Kavukcuoglu},
      year={2018},
      eprint={1802.01561},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{torchbeast,
      title={TorchBeast: A PyTorch Platform for Distributed RL}, 
      author={Heinrich Küttler and Nantas Nardelli and Thibaut Lavril and Marco Selvatici and Viswanath Sivakumar and Tim Rocktäschel and Edward Grefenstette},
      year={2019},
      eprint={1910.03552},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{RND,
      title={Exploration by Random Network Distillation}, 
      author={Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
      year={2018},
      eprint={1810.12894},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}